---
title: "R Notebook"
output: html_notebook
---

# About the notebook
```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(caret)
library(class)        ### for knn
library(glmnet)       ### for LASSO/Ridge
library(randomForest) ### for rf
library(gridExtra)
```

First I will write the cross validation for each methods

# Import data

read in data
```{r, message = FALSE, warning = FALSE}
dat_raw = read_csv("./data/data.csv")
dat_raw = dat_raw[, 1:32]
colnames(dat_raw) = str_replace(
    string = colnames(dat_raw), 
    pattern = " ", 
    replacement = "_")

dat_raw$diagnosis = factor(dat_raw$diagnosis, levels = c("B", "M"))
dat_raw$label     = as.integer(dat_raw$diagnosis) - 1
```

check if the label is balanced
```{r}
table(dat_raw$diagnosis)
```

upsampleing to make the label balance
```{r}
### initialization
set.seed(123)

### upsampleing of malignant tumor
numdiff = table(dat_raw$diagnosis)["B"] - table(dat_raw$diagnosis)["M"]
dat = dat_raw %>% filter(diagnosis == "M") %>% sample_n(size = numdiff) 
dat = bind_rows(dat_raw, dat)

### check if the label is balanced
table(dat$diagnosis)
```


preprocess the data
```{r}
dat_prep  = dat  %>% select(-id, -diagnosis)
dat_covar = dat_prep %>% select(-label)
dat_label = dat_prep %>% select( label)
```

# helper function for cross validation
```{r}
rf_cv = function(dat, mtrys, nodesizes, ntree = 10, K = 2) {
    ### cartesian product of two parameters
    params = expand.grid(mtry = mtrys, nodesize = nodesizes)
    
    ### init cv
    sp  = split(1:nrow(dat), 1:K)    
    err = matrix(NA, nrow = nrow(dat), ncol = nrow(params))
    
    ### perform cross validation (cv)
    for (k in 1:K) {
        ### get the train test data for cv
        dat_train = dat[-sp[[k]], ]
        dat_test  = dat[ sp[[k]], ]
    
        ###
        for (idx in seq_len(nrow(params))){
            param  = params[idx, ]
            mtry   = param$mtry
            ndsize = param$nodesize
    
            rf <- randomForest(
                factor(label) ~ ., data = dat_train, 
                importance = T, 
                mtry       = mtry,
                nodesize   = ndsize,
                ntree      = ntree)
    
            err[sp[[k]], idx] = (dat_test$label != predict(rf, dat_test, type = "response"))
        } # end for loop
    } # end outer loop 
    
    ### get the model with 
    tmp = apply(err, 2, sum)
    idx = which.min(tmp)
    res = list(params = params, 
               mtry   = params[idx, ]$mtry,
               ndsize = params[idx, ]$nodesize)
    return(res)
} # end func

### check if the function work without any error
dat       = dat_covar %>% scale %>% as.data.frame
dat$label = dat_label$label

MTRY_ALL     = c(1, 5, 10, 15, 20)
NODESIZE_ALL = c(5, 10, 15, 20)

res_rf = rf_cv(dat, mtrys = MTRY_ALL, nodesizes = NODESIZE_ALL, ntree = 20, K = 6)
res_rf$mtry
res_rf$ndsize
```

```{r}
knn_cv = function(dat, knears, K = 2) {
    ### init cv
    sp  = split(1:nrow(dat), 1:K)    
    err = matrix(NA, nrow = nrow(dat), ncol = length(knears))
    
    ### perform cross validation (cv)
    for (k in 1:K) {
        ### get the train test data for cv
        dat_train = dat[-sp[[k]], ]
        dat_test  = dat[ sp[[k]], ]
        
        x_train = dat_train %>% select(-label)
        x_test  = dat_test  %>% select(-label)
        y_train = dat_train$label
        y_test  = dat_test$label
            
        ### knn 
        for (idx in seq_along(knears)){
            knear = knears[idx]
            yhat  = knn(train = x_train, test = x_test, cl = y_train, k = knear)
            
            err[sp[[k]], idx] = (y_test != yhat)
        } # end for loop
    } # end outer loop 
    
    ### get the model with 
    tmp = apply(err, 2, sum)
    idx = which.min(tmp)
    return(knears[idx])
} # end func

### check if the function work without any error
dat       = dat_covar %>% scale %>% as.data.frame
dat$label = dat_label$label

KNEARS    = c(2, 3, 4, 5, 6, 7, 8)
knn_cv(dat, knears = KNEARS, K = 14)
```


# cross validation of all methods

```{r, warning=FALSE}
### init cv
KNEARS       = c(1, 2, 3, 4, 5, 6, 7, 8)
MTRY_ALL     = c(1,  5, 10, 20)
NODESIZE_ALL = c(5, 10, 15, 20)
NTREE        = 50
METHODS      = c("lasso", "rf", "knn")
K_OUTTER     = 10
K_INNTER     = 10

### split data
sp  = split(1:nrow(dat), 1:K)    
err = matrix(NA, nrow = nrow(dat), ncol = length(METHODS))

### perform cross validation (cv)
for (k in 1:K) {
    ### get the train test data for cv
    dat_train = dat[-sp[[k]], ]
    dat_test  = dat[ sp[[k]], ]
    
    ###
    x_train = dat_train %>% select(-label) %>% scale
    x_test  = dat_test  %>% select(-label) %>% scale
    y_train = dat_train$label
    y_test  = dat_test$label
        
    ###
    for (idx in seq_along(METHODS)){
        ###
        method = METHODS[idx]
        
        ### 
        if (method == "knn") {
            ### cross validation to find the best k in knn
            knear = knn_cv(dat_train, KNEARS, K = K_INNTER)
            knear = KNEARS[idx]
            yhat  = knn(train = x_train, test = x_test, cl = y_train, k = knear)
            err[sp[[k]], idx] = (y_test != yhat)
        }
        
        if (method == "rf") {
            ### cross validation to find the best mtry and nodesize in random forest
            res_rf = rf_cv(dat_train, mtrys = MTRY_ALL, nodesizes = NODESIZE_ALL, ntree = NTREE, K = K_INNTER)
            mtry   = res_rf$mtry
            ndsize = res_rf$ndsize
            
            rf = randomForest(
                factor(label) ~ ., data = dat_train, 
                importance = T, 
                mtry       = mtry,
                nodesize   = ndsize,
                ntree      = NTREE)
            
            yhat = predict(rf, dat_test, type = "response")
            err[sp[[k]], idx] = (y_test != yhat)
        }
        
        if (method == "lasso") {
            ### cross validation to find the best lambda in LASSO
            lasFit_cv <- cv.glmnet(
                x = x_train, 
                y = factor(y_train), 
                family = "binomial", 
                alpha  = 1, 
                nfolds = K_INNTER)
            
            yhat = predict(lasFit_cv, newx = x_test, s = "lambda.1se", type="class")
            yhat = as.integer(yhat)
            err[sp[[k]], idx] = (y_test != yhat)
        }
        
    } # end for loop
} # end outer loop 
    
### get the model with 
tmp = apply(err, 2, sum)
names(tmp) = METHODS
print("Misclassification error in CV")
print(tmp)

idx = which.min(tmp)
print("Best method")
print(METHODS[idx])
```




